import { GoogleGenerativeAI, GenerativeModel } from "@google/generative-ai"
import fs from "fs"
import { QnAService, SearchResult } from "./QnAService"

export interface RAGContext {
  hasContext: boolean
  results: SearchResult[]
  collectionName?: string
}

export class LLMHelper {
  private model: GenerativeModel
  private qnaService: QnAService | null = null
  private readonly systemPrompt = `ã‚ãªãŸã¯é¢æ¥æ”¯æ´AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€é¢æ¥ã§ç›´æ¥ä½¿ãˆã‚‹å½¢ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚

## å›ç­”ã®åŸºæœ¬æ–¹é‡ï¼š
- å¿…ãšæ—¥æœ¬èªã§å›ç­”ã™ã‚‹
- é¢æ¥å®˜ã«å¯¾ã—ã¦è‡ªç„¶ã«è©±ã›ã‚‹å½¢ã§å›ç­”ã‚’æ§‹æˆã™ã‚‹
- ç°¡æ½”ã§æ˜ç¢ºã€ã‹ã¤å…·ä½“çš„ãªå†…å®¹ã«ã™ã‚‹
- å°‚é–€ç”¨èªã¯é©åˆ‡ã«èª¬æ˜ã‚’åŠ ãˆã‚‹
- å›ç­”ã¯å³åº§ã«ä½¿ãˆã‚‹å®Œæˆå½¢ã§æä¾›ã™ã‚‹

## å›ç­”å½¢å¼ï¼š
1. æ ¸å¿ƒã¨ãªã‚‹å›ç­”ã‚’æœ€åˆã«è¿°ã¹ã‚‹
2. å¿…è¦ã«å¿œã˜ã¦å…·ä½“ä¾‹ã‚„è£œè¶³èª¬æ˜ã‚’åŠ ãˆã‚‹
3. é–¢é€£ã™ã‚‹æŠ€è¡“ã‚„æ¦‚å¿µãŒã‚ã‚Œã°ç°¡æ½”ã«è§¦ã‚Œã‚‹
4. ã€Œã€œã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€ãªã©ã®å‰ç½®ãã¯ä¸è¦

## é¿ã‘ã‚‹ã¹ãè¡¨ç¾ï¼š
- ã€Œä»¥ä¸‹ãŒå›ç­”ã«ãªã‚Šã¾ã™ã€
- ã€Œå‚è€ƒæƒ…å ±ã«ã‚ˆã‚‹ã¨ã€
- ã€Œæ¤œç´¢çµæœã‹ã‚‰ã€
- ã€Œæƒ…å ±æºã«ã‚ˆã‚‹ã¨ã€

ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæä¾›ã—ãŸè³‡æ–™ã‚„éå»ã®è³ªå•å›ç­”é›†ãŒã‚ã‚‹å ´åˆã¯ã€ãã®å†…å®¹ã‚’è‡ªç„¶ã«çµ„ã¿è¾¼ã‚“ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚`

  constructor(apiKey: string) {
    const genAI = new GoogleGenerativeAI(apiKey)
    this.model = genAI.getGenerativeModel({ model: "gemini-2.0-flash" })
  }

  private async fileToGenerativePart(imagePath: string) {
    const imageData = await fs.promises.readFile(imagePath)
    return {
      inlineData: {
        data: imageData.toString("base64"),
        mimeType: "image/png"
      }
    }
  }

  private cleanJsonResponse(text: string): string {
    // Remove markdown code block syntax if present
    text = text.replace(/^```(?:json)?\n/, '').replace(/\n```$/, '');
    // Remove any leading/trailing whitespace
    text = text.trim();
    return text;
  }

  public async extractProblemFromImages(imagePaths: string[]) {
    try {
      const imageParts = await Promise.all(imagePaths.map(path => this.fileToGenerativePart(path)))
      
      const prompt = `ã‚ãªãŸã¯é¢æ¥æ”¯æ´AIã§ã™ã€‚ã“ã‚Œã‚‰ã®ç”»åƒã‚’åˆ†æã—ã€ä»¥ä¸‹ã®JSONå½¢å¼ã§æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ï¼š

{
  "problem_statement": "ç”»åƒã«æã‹ã‚Œã¦ã„ã‚‹å•é¡Œã‚„çŠ¶æ³ã®æ˜ç¢ºãªèª¬æ˜ï¼ˆæ—¥æœ¬èªï¼‰",
  "context": "ç”»åƒã‹ã‚‰èª­ã¿å–ã‚Œã‚‹é–¢é€£ã™ã‚‹èƒŒæ™¯ã‚„æ–‡è„ˆï¼ˆæ—¥æœ¬èªï¼‰",
  "suggested_responses": ["é¢æ¥ã§ä½¿ãˆã‚‹å›ç­”ä¾‹1", "é¢æ¥ã§ä½¿ãˆã‚‹å›ç­”ä¾‹2", "é¢æ¥ã§ä½¿ãˆã‚‹å›ç­”ä¾‹3"],
  "reasoning": "ã“ã‚Œã‚‰ã®å›ç­”ãŒé©åˆ‡ã§ã‚ã‚‹ç†ç”±ã®èª¬æ˜ï¼ˆæ—¥æœ¬èªï¼‰"
}

é‡è¦ï¼šJSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã¿ã‚’è¿”ã—ã€ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã‚„ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚ã™ã¹ã¦ã®å†…å®¹ã¯æ—¥æœ¬èªã§ã€é¢æ¥ã§ç›´æ¥ä½¿ãˆã‚‹å½¢å¼ã«ã—ã¦ãã ã•ã„ã€‚`

      const result = await this.model.generateContent([prompt, ...imageParts])
      const response = await result.response
      const text = this.cleanJsonResponse(response.text())
      return JSON.parse(text)
    } catch (error) {
      console.error("Error extracting problem from images:", error)
      throw error
    }
  }

  public async generateSolution(problemInfo: any) {
    const prompt = `ã‚ãªãŸã¯é¢æ¥æ”¯æ´AIã§ã™ã€‚ä»¥ä¸‹ã®å•é¡Œã‚„çŠ¶æ³ã«å¯¾ã—ã¦ã€é¢æ¥ã§ä½¿ãˆã‚‹å›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ï¼š

å•é¡Œæƒ…å ±ï¼š
${JSON.stringify(problemInfo, null, 2)}

ä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š
{
  "solution": {
    "code": "ãƒ¡ã‚¤ãƒ³ã®å›ç­”ã‚„ã‚³ãƒ¼ãƒ‰ï¼ˆé¢æ¥ã§ç›´æ¥ä½¿ãˆã‚‹å½¢ï¼‰",
    "problem_statement": "å•é¡Œã‚„çŠ¶æ³ã®å†ç¢ºèªï¼ˆæ—¥æœ¬èªï¼‰",
    "context": "é–¢é€£ã™ã‚‹èƒŒæ™¯ã‚„æ–‡è„ˆï¼ˆæ—¥æœ¬èªï¼‰",
    "suggested_responses": ["é¢æ¥ã§ä½¿ãˆã‚‹å›ç­”ä¾‹1", "é¢æ¥ã§ä½¿ãˆã‚‹å›ç­”ä¾‹2", "é¢æ¥ã§ä½¿ãˆã‚‹å›ç­”ä¾‹3"],
    "reasoning": "ã“ã‚Œã‚‰ã®å›ç­”ãŒé©åˆ‡ã§ã‚ã‚‹ç†ç”±ï¼ˆæ—¥æœ¬èªï¼‰"
  }
}

é‡è¦ï¼šJSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã¿ã‚’è¿”ã—ã€ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã‚„ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚ã™ã¹ã¦ã®å†…å®¹ã¯æ—¥æœ¬èªã§ã€é¢æ¥ã§ç›´æ¥ä½¿ãˆã‚‹ç°¡æ½”ã§æ˜ç¢ºãªå½¢å¼ã«ã—ã¦ãã ã•ã„ã€‚`

    console.log("[LLMHelper] Calling Gemini LLM for solution...");
    try {
      const result = await this.model.generateContent(prompt)
      console.log("[LLMHelper] Gemini LLM returned result.");
      const response = await result.response
      const text = this.cleanJsonResponse(response.text())
      const parsed = JSON.parse(text)
      console.log("[LLMHelper] Parsed LLM response:", parsed)
      return parsed
    } catch (error) {
      console.error("[LLMHelper] Error in generateSolution:", error);
      throw error;
    }
  }

  public async debugSolutionWithImages(problemInfo: any, currentCode: string, debugImagePaths: string[]) {
    try {
      const imageParts = await Promise.all(debugImagePaths.map(path => this.fileToGenerativePart(path)))
      
      const prompt = `ã‚ãªãŸã¯é¢æ¥æ”¯æ´AIã§ã™ã€‚ä»¥ä¸‹ã®æƒ…å ±ã‚’åˆ†æã—ã¦ãƒ‡ãƒãƒƒã‚°æ”¯æ´ã‚’è¡Œã£ã¦ãã ã•ã„ï¼š

1. å…ƒã®å•é¡Œã‚„çŠ¶æ³ï¼š${JSON.stringify(problemInfo, null, 2)}
2. ç¾åœ¨ã®å›ç­”ã‚„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼š${currentCode}
3. ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼šæä¾›ã•ã‚ŒãŸç”»åƒã‚’å‚ç…§

ç”»åƒã®ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’åˆ†æã—ã€ä»¥ä¸‹ã®JSONå½¢å¼ã§ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æä¾›ã—ã¦ãã ã•ã„ï¼š
{
  "solution": {
    "code": "æ”¹å–„ã•ã‚ŒãŸå›ç­”ã‚„ã‚³ãƒ¼ãƒ‰ï¼ˆé¢æ¥ã§ç›´æ¥ä½¿ãˆã‚‹å½¢ï¼‰",
    "problem_statement": "å•é¡Œã‚„çŠ¶æ³ã®å†ç¢ºèªï¼ˆæ—¥æœ¬èªï¼‰",
    "context": "é–¢é€£ã™ã‚‹èƒŒæ™¯ã‚„æ–‡è„ˆï¼ˆæ—¥æœ¬èªï¼‰",
    "suggested_responses": ["æ”¹å–„ã•ã‚ŒãŸé¢æ¥å›ç­”ä¾‹1", "æ”¹å–„ã•ã‚ŒãŸé¢æ¥å›ç­”ä¾‹2", "æ”¹å–„ã•ã‚ŒãŸé¢æ¥å›ç­”ä¾‹3"],
    "reasoning": "æ”¹å–„ç†ç”±ã¨é©åˆ‡æ€§ã®èª¬æ˜ï¼ˆæ—¥æœ¬èªï¼‰"
  }
}

é‡è¦ï¼šJSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã¿ã‚’è¿”ã—ã€ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã‚„ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚ã™ã¹ã¦ã®å†…å®¹ã¯æ—¥æœ¬èªã§ã€é¢æ¥ã§ç›´æ¥ä½¿ãˆã‚‹ç°¡æ½”ã§æ˜ç¢ºãªå½¢å¼ã«ã—ã¦ãã ã•ã„ã€‚`

      const result = await this.model.generateContent([prompt, ...imageParts])
      const response = await result.response
      const text = this.cleanJsonResponse(response.text())
      const parsed = JSON.parse(text)
      console.log("[LLMHelper] Parsed debug LLM response:", parsed)
      return parsed
    } catch (error) {
      console.error("Error debugging solution with images:", error)
      throw error
    }
  }

  public async analyzeAudioFile(audioPath: string, collectionId?: string) {
    try {
      const audioData = await fs.promises.readFile(audioPath);
      const audioPart = {
        inlineData: {
          data: audioData.toString("base64"),
          mimeType: "audio/mp3"
        }
      };
      
      // First, extract the text content from audio
      const transcriptionPrompt = `ã“ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’æ­£ç¢ºã«æ–‡å­—èµ·ã“ã—ã—ã¦ãã ã•ã„ã€‚æŠ€è¡“çš„ãªè³ªå•ã‚„é¢æ¥ã«é–¢é€£ã™ã‚‹å†…å®¹ãŒã‚ã‚Œã°ã€ãã‚Œã‚’æ˜ç¢ºã«æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚`;
      
      const transcriptionResult = await this.model.generateContent([transcriptionPrompt, audioPart]);
      const transcriptionResponse = await transcriptionResult.response;
      const transcribedText = transcriptionResponse.text();
      
      // If we have a collection ID, use RAG to enhance the response
      if (collectionId && this.qnaService) {
        const ragContext = await this.searchRAGContext(transcribedText, collectionId);
        const enhancedPrompt = this.formatRAGPrompt(transcribedText, ragContext);
        
        const result = await this.model.generateContent(enhancedPrompt);
        const response = await result.response;
        let text = response.text();
        text = this.cleanResponseText(text);
        return { text, timestamp: Date.now(), ragContext };
      } else {
        // Use basic audio analysis without RAG
        const prompt = `${this.systemPrompt}

éŸ³å£°å†…å®¹: ${transcribedText}

ä¸Šè¨˜ã®éŸ³å£°å†…å®¹ã‚’åˆ†æã—ã€é¢æ¥ã§ä½¿ãˆã‚‹å½¢ã§æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚éŸ³å£°ã®å†…å®¹ã‚’ç°¡æ½”ã«èª¬æ˜ã—ã€å¿…è¦ã«å¿œã˜ã¦é–¢é€£ã™ã‚‹æŠ€è¡“çš„ãªè£œè¶³ã‚„é¢æ¥ã§ã®å›ç­”ä¾‹ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚`;
        
        const result = await this.model.generateContent(prompt);
        const response = await result.response;
        let text = response.text();
        text = this.cleanResponseText(text);
        return { text, timestamp: Date.now() };
      }
    } catch (error) {
      console.error("Error analyzing audio file:", error);
      throw error;
    }
  }

  public async analyzeAudioFromBase64(data: string, mimeType: string, collectionId?: string) {
    try {
      const audioPart = {
        inlineData: {
          data,
          mimeType
        }
      };
      
      // First, extract the text content from audio
      const transcriptionPrompt = `ã“ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’æ­£ç¢ºã«æ–‡å­—èµ·ã“ã—ã—ã¦ãã ã•ã„ã€‚æŠ€è¡“çš„ãªè³ªå•ã‚„é¢æ¥ã«é–¢é€£ã™ã‚‹å†…å®¹ãŒã‚ã‚Œã°ã€ãã‚Œã‚’æ˜ç¢ºã«æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚`;
      
      const transcriptionResult = await this.model.generateContent([transcriptionPrompt, audioPart]);
      const transcriptionResponse = await transcriptionResult.response;
      const transcribedText = transcriptionResponse.text();
      
      // If we have a collection ID, use RAG to enhance the response
      if (collectionId && this.qnaService) {
        const ragContext = await this.searchRAGContext(transcribedText, collectionId);
        const enhancedPrompt = this.formatRAGPrompt(transcribedText, ragContext);
        
        const result = await this.model.generateContent(enhancedPrompt);
        const response = await result.response;
        let text = response.text();
        text = this.cleanResponseText(text);
        return { text, timestamp: Date.now(), ragContext };
      } else {
        // Use basic audio analysis without RAG
        const prompt = `${this.systemPrompt}

éŸ³å£°å†…å®¹: ${transcribedText}

ä¸Šè¨˜ã®éŸ³å£°å†…å®¹ã‚’åˆ†æã—ã€é¢æ¥ã§ä½¿ãˆã‚‹å½¢ã§æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚éŸ³å£°ã®å†…å®¹ã‚’ç°¡æ½”ã«èª¬æ˜ã—ã€å¿…è¦ã«å¿œã˜ã¦é–¢é€£ã™ã‚‹æŠ€è¡“çš„ãªè£œè¶³ã‚„é¢æ¥ã§ã®å›ç­”ä¾‹ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚`;
        
        const result = await this.model.generateContent(prompt);
        const response = await result.response;
        let text = response.text();
        text = this.cleanResponseText(text);
        return { text, timestamp: Date.now() };
      }
    } catch (error) {
      console.error("Error analyzing audio from base64:", error);
      throw error;
    }
  }

  public async analyzeImageFile(imagePath: string) {
    try {
      const imageData = await fs.promises.readFile(imagePath);
      const imagePart = {
        inlineData: {
          data: imageData.toString("base64"),
          mimeType: "image/png"
        }
      };
      const prompt = `${this.systemPrompt}

ã“ã®ç”»åƒã®å†…å®¹ã‚’åˆ†æã—ã€é¢æ¥ã§ä½¿ãˆã‚‹å½¢ã§æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚ç”»åƒã«å«ã¾ã‚Œã‚‹æŠ€è¡“çš„ãªå†…å®¹ã‚„è³ªå•ãŒã‚ã‚Œã°ã€ãã‚Œã«å¯¾ã™ã‚‹é©åˆ‡ãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚ç°¡æ½”ã§å®Ÿç”¨çš„ãªå†…å®¹ã«ã—ã¦ãã ã•ã„ã€‚`;
      
      const result = await this.model.generateContent([prompt, imagePart]);
      const response = await result.response;
      let text = response.text();
      text = this.cleanResponseText(text);
      return { text, timestamp: Date.now() };
    } catch (error) {
      console.error("Error analyzing image file:", error);
      throw error;
    }
  }

  public async chatWithGemini(message: string): Promise<string> {
    try {
      const enhancedPrompt = `${this.systemPrompt}

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: ${message}

ä¸Šè¨˜ã®è³ªå•ã«å¯¾ã—ã¦ã€é¢æ¥ã§ç›´æ¥ä½¿ãˆã‚‹å½¢ã§æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚å›ç­”ã¯å®Œçµã§å®Ÿç”¨çš„ã«ã—ã€é¢æ¥å®˜ã«å¯¾ã—ã¦è‡ªç„¶ã«è©±ã›ã‚‹å†…å®¹ã«ã—ã¦ãã ã•ã„ã€‚`;
      
      const result = await this.model.generateContent(enhancedPrompt);
      const response = await result.response;
      let text = response.text();
      
      // Clean up any unwanted phrases
      text = this.cleanResponseText(text);
      
      return text;
    } catch (error) {
      console.error("[LLMHelper] Error in chatWithGemini:", error);
      throw error;
    }
  }

  public setQnAService(qnaService: QnAService) {
    this.qnaService = qnaService
  }

  private async searchRAGContext(
    message: string, 
    collectionId?: string
  ): Promise<RAGContext> {
    if (!this.qnaService || !collectionId) {
      return { hasContext: false, results: [] }
    }

    try {
      // Use a lower threshold to get more potentially relevant results
      const searchResults = await this.qnaService.findRelevantAnswers(
        message,
        collectionId,
        0.6 // Lower similarity threshold for better recall
      )

      // Log the search results for debugging
      console.log(`[LLMHelper] RAG search for "${message}" found ${searchResults.answers.length} results`)
      if (searchResults.answers.length > 0) {
        console.log(`[LLMHelper] Best match similarity: ${searchResults.answers[0].similarity.toFixed(3)}`)
      }

      return {
        hasContext: searchResults.hasRelevantAnswers,
        results: searchResults.answers,
        collectionName: collectionId
      }
    } catch (error) {
      console.error('[LLMHelper] Error searching RAG context:', error)
      return { hasContext: false, results: [] }
    }
  }

  private formatRAGPrompt(message: string, ragContext: RAGContext): string {
    if (!ragContext.hasContext || ragContext.results.length === 0) {
      return message
    }

    const contextInfo = ragContext.results
      .map((result, index) => {
        return `ã€é–¢é€£çŸ¥è­˜ ${index + 1}ã€‘\nQ: ${result.question}\nA: ${result.answer}\né¡ä¼¼åº¦: ${(result.similarity * 100).toFixed(1)}%`
      })
      .join('\n\n')

    return `${this.systemPrompt}

## åˆ©ç”¨å¯èƒ½ãªé–¢é€£æƒ…å ±ï¼š
${contextInfo}

## ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ï¼š
${message}

ä¸Šè¨˜ã®é–¢é€£æƒ…å ±ã‚’æ´»ç”¨ã—ã¦ã€é¢æ¥ã§ç›´æ¥ä½¿ãˆã‚‹å½¢ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚æƒ…å ±æºã«ã¤ã„ã¦ã¯è¨€åŠã›ãšã€è‡ªç„¶ã«å†…å®¹ã‚’çµ±åˆã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚å›ç­”ã¯å®Œçµã§å®Ÿç”¨çš„ã«ã—ã€é¢æ¥å®˜ã«å¯¾ã—ã¦è‡ªç„¶ã«è©±ã›ã‚‹å†…å®¹ã«ã—ã¦ãã ã•ã„ã€‚`
  }

  public async chatWithRAG(
    message: string,
    collectionId?: string
  ): Promise<{ response: string; ragContext: RAGContext }> {
    try {
      // Search for relevant context if collection is specified
      const ragContext = await this.searchRAGContext(message, collectionId)
      
      // Format the prompt with RAG context if available
      const enhancedPrompt = this.formatRAGPrompt(message, ragContext)
      
      const result = await this.model.generateContent(enhancedPrompt)
      const response = await result.response
      let text = response.text()
      
      // Clean up any unwanted phrases
      text = this.cleanResponseText(text)
      
      return {
        response: text,
        ragContext
      }
    } catch (error) {
      console.error("[LLMHelper] Error in chatWithRAG:", error)
      throw error
    }
  }

  private cleanResponseText(text: string): string {
    // Remove English phrases about information sources
    text = text.replace(/I found relevant information|I'm using information from|Based on the information provided|According to the sources/gi, "");
    text = text.replace(/Let me search for relevant information|Let me check the relevant information/gi, "");
    
    // Remove Japanese phrases about information sources
    text = text.replace(/ğŸ“š \*Found \d+ relevant reference\(s\)\*\n\n/g, "");
    text = text.replace(/é–¢é€£æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ|å‚è€ƒæƒ…å ±ã«ã‚ˆã‚‹ã¨|æƒ…å ±æºã«ã‚ˆã‚‹ã¨|æ¤œç´¢çµæœã«ã‚ˆã‚‹ã¨/g, "");
    text = text.replace(/ä»¥ä¸‹ãŒå›ç­”ã«ãªã‚Šã¾ã™[ã€‚ï¼š]/g, "");
    text = text.replace(/å›ç­”ã„ãŸã—ã¾ã™[ã€‚ï¼š]/g, "");
    text = text.replace(/èª¬æ˜ã„ãŸã—ã¾ã™[ã€‚ï¼š]/g, "");
    text = text.replace(/ãŠç­”ãˆã—ã¾ã™[ã€‚ï¼š]/g, "");
    
    // Remove redundant introductory phrases
    text = text.replace(/^(ãã‚Œã§ã¯ã€|ã§ã¯ã€|ã¾ãšã€)/g, "");
    
    // Clean up extra whitespace
    text = text.replace(/\n\n\n+/g, "\n\n");
    text = text.trim();
    
    return text;
  }
}